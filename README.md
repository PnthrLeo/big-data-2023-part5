# Модель кластеризации на PySpark

## Цель работы
Целью данной работы является получение навыков разработки и настройки Spark приложения.

## Выполнение работы
1. Была настроена среда для Spark вычислений. Для этого были написаны Dockerfile и docker-compose.yml файлы.
2. Была проверена работоспособность среды при помощи скрипта `src/wordcount.py`.
3. Был написан скрипты для фильтрации данных и обучения модели кластеризации.
4. Был осуществлён подбор параметров конфигурации Spark приложения на основе [руководства](https://habr.com/ru/companies/otus/articles/529100/):
    1.  Так как всего у домашней системы 12 логических процессоров, то было решено выделить 10 ядер для Spark приложения (12 логических ядер - 1 ядро под операционную систему - 1 ядро для кратности).
    2. В руководстве отмечалось, что 5 ядер на 1 исполнитель - оптимальное соотношение. Поэтому было решено выделить 2 исполнителя (`spark.num.executors = 2, spark.executor.cores = 5`).
    3. Так как в системе 16 Гб оперативной памяти, то было решено выделить 12 Гб для Spark приложения (16 Гб - 4 Гб для операционной системы). Соответственно, по формуле из руководства значение параметра `spark.executor.memory` равно $12 \div 2 \div 1.1 \approx 5$ Гб.
    4. Значение `spark.driver.memory` взято равно `spark.executor.memory`, то есть 5 Гб.

## Запуск среды
Для запуска среды нужно выполнить следующие команды из корня репозитория:
1. `docker-compose build`
2. `docker-compose up -d`
3. `docker exec -i -t [container id] bash`

Далее, для запуска Jupyter Notebook (ноутбук будет доступен на порту 8888):
1. `jupyter notebook --ip 0.0.0.0 --no-browser --allow-root`

Для запуска скриптов нужно перейти в папку src Docker контейнера и выполнить команду:
1. `python [script name]`

Во время запущенного Jupyter Notebook вы можете перейти по адресу `localhost:4444` для доступа к Spark веб-интерфейсу.
Также доступ можно получить и при запуске `src`-кода, для этого нужно добавить вызов функции `keep_spark_web_ui_alive` из `utils.spark` перед закрытием `SparkSession`.


## Результаты работы
1. Была получена среда для Spark вычислений.
2. Была написана модель KMeans кластеризации на pySpark.
3. Были подобраны параметры конфигурации Spark приложения оптимальные для конкретной системы.
